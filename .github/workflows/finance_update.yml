name: Yearly Finanace Data Fetch and Upload

on:
  schedule:
    # Runs at 00:00 on July 20th every year
    - cron: '0 0 20 7 *'
  workflow_dispatch: # Allows manual testing

jobs:
  fetch-and-upload:
    runs-on: ubuntu-latest
    strategy:
      # This creates 10 parallel instances of the job
      fail-fast: false  # 关键：即使一组失败，其他 9 组也会继续运行
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    # 设置单个任务的超时时间（例如 30 分钟），防止无限挂起
    timeout-minutes: 30
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip' # 开启缓存，减少重复安装出错的概率
      - name: Install Dependencies
        run: |
          pip install huggingface_hub
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        # 如果安装经常失败，可以添加重试逻辑
        continue-on-error: false
      - name: Run Fetch Script
        env:
          GROUP_ID: ${{ matrix.group }}
        run: |
          # The script creates /output and /log internally
          python scripts/fetch_data.py $(( ${{ matrix.group }} - 1 ))

      - name: Upload Data & Logs to HF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          from huggingface_hub import HfApi
          import os
          api = HfApi()
          repo = 'langwnwk/stock_finance'
          token = os.environ['HF_TOKEN']
          group_tag = 'group_${{ matrix.group }}'

          # 1. 上传数据文件
          if os.path.exists('output') and os.listdir('output'):
              api.upload_folder(
                  folder_path='output',
                  repo_id=repo,
                  repo_type='dataset',
                  path_in_repo=f'latest_data/{group_tag}',
                  token=token,
                  delete_patterns=['*']
              )

          # 2. 上传失败日志 (供明日补漏任务读取)
          if os.path.exists('log') and os.listdir('log'):
              api.upload_folder(
                  folder_path='log',
                  repo_id=repo,
                  repo_type='dataset',
                  path_in_repo=f'latest_logs/{group_tag}',
                  token=token,
                  delete_patterns=['*']
              )
          "
