name: Yearly Data Fetch and Upload

on:
  schedule:
    # Runs at 00:00 on July 20th every year
    - cron: '0 0 20 7 *'
  workflow_dispatch: # Allows manual testing

jobs:
  fetch-and-upload:
    runs-on: ubuntu-latest
    strategy:
      # This creates 10 parallel instances of the job
      matrix:
        group: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          pip install huggingface_hub
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run Fetch Script
        env:
          GROUP_ID: ${{ matrix.group }}
        run: |
          # The script creates /output and /log internally
          python scripts/fetch_data.py --group $GROUP_ID

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          from huggingface_hub import HfApi
          api = HfApi()
          api.upload_folder(
              folder_path='output',
              repo_id='your-username/your-dataset-name',
              repo_type='dataset',
              token='${{ secrets.HF_TOKEN }}',
              path_in_repo=f'year_2026/group_${{ matrix.group }}'
          )"
